{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block allows us to import from the benchmark folder,\n",
    "# as if it was a package installed using pip\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import datasets, methods, models\n",
    "from benchmark.infidelity import infidelity\n",
    "import itertools\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General parameters\n",
    "batch_size = 16\n",
    "n_batches = 16\n",
    "n_perturbations = 1\n",
    "data_root = \"../data\"\n",
    "use_logits = True\n",
    "dataset_name = \"CIFAR10\"\n",
    "normalize_attrs = True\n",
    "aggregation_fn = \"avg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Resnet:\n\tMissing key(s) in state_dict: \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\". \n\tUnexpected key(s) in state_dict: \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.bn1.num_batches_tracked\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.bn2.num_batches_tracked\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.bn1.num_batches_tracked\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.bn2.num_batches_tracked\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.bn1.num_batches_tracked\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.bn2.num_batches_tracked\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).\n\tsize mismatch for bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer1.0.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer1.0.bn2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.bn2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.bn2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.bn2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer1.1.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer1.1.bn2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.bn2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.bn2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.bn2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer2.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for layer2.0.bn1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.bn1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.bn1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.bn1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer2.0.bn2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.bn2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.bn2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.bn2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for layer2.0.downsample.1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer2.1.bn1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.bn1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.bn1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.bn1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer2.1.bn2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.bn2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.bn2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.bn2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer3.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for layer3.0.bn1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.bn1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.bn1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.bn1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer3.0.bn2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.bn2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.bn2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.bn2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for layer3.0.downsample.1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer3.1.bn1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.bn1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.bn1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.bn1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer3.1.bn2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.bn2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.bn2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.bn2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 64]) from checkpoint, the shape in current model is torch.Size([10, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-5ff6de288614>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mdataset_name\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"CIFAR10\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0mdataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCifar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_location\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_root\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"CIFAR10\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdownload\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshuffle\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mversion\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"cifar10\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mResnet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mversion\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"resnet18\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutput_logits\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0muse_logits\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_classes\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams_loc\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_root\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"models/CIFAR10_resnet20.pth\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meval\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/PhD/projects/benchmark/notebooks/util/models/resnet.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, version, output_logits, num_classes, params_loc)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mparams_loc\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 31\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_state_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams_loc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmap_location\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mlambda\u001B[0m \u001B[0mstorage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mloc\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mstorage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     32\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget_logits\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Documents/PhD/projects/benchmark/venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36mload_state_dict\u001B[0;34m(self, state_dict, strict)\u001B[0m\n\u001B[1;32m    828\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merror_msgs\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    829\u001B[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001B[0;32m--> 830\u001B[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001B[0m\u001B[1;32m    831\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_IncompatibleKeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmissing_keys\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0munexpected_keys\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    832\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for Resnet:\n\tMissing key(s) in state_dict: \"layer4.0.conv1.weight\", \"layer4.0.bn1.weight\", \"layer4.0.bn1.bias\", \"layer4.0.bn1.running_mean\", \"layer4.0.bn1.running_var\", \"layer4.0.conv2.weight\", \"layer4.0.bn2.weight\", \"layer4.0.bn2.bias\", \"layer4.0.bn2.running_mean\", \"layer4.0.bn2.running_var\", \"layer4.0.downsample.0.weight\", \"layer4.0.downsample.1.weight\", \"layer4.0.downsample.1.bias\", \"layer4.0.downsample.1.running_mean\", \"layer4.0.downsample.1.running_var\", \"layer4.1.conv1.weight\", \"layer4.1.bn1.weight\", \"layer4.1.bn1.bias\", \"layer4.1.bn1.running_mean\", \"layer4.1.bn1.running_var\", \"layer4.1.conv2.weight\", \"layer4.1.bn2.weight\", \"layer4.1.bn2.bias\", \"layer4.1.bn2.running_mean\", \"layer4.1.bn2.running_var\". \n\tUnexpected key(s) in state_dict: \"layer1.2.conv1.weight\", \"layer1.2.bn1.weight\", \"layer1.2.bn1.bias\", \"layer1.2.bn1.running_mean\", \"layer1.2.bn1.running_var\", \"layer1.2.bn1.num_batches_tracked\", \"layer1.2.conv2.weight\", \"layer1.2.bn2.weight\", \"layer1.2.bn2.bias\", \"layer1.2.bn2.running_mean\", \"layer1.2.bn2.running_var\", \"layer1.2.bn2.num_batches_tracked\", \"layer2.2.conv1.weight\", \"layer2.2.bn1.weight\", \"layer2.2.bn1.bias\", \"layer2.2.bn1.running_mean\", \"layer2.2.bn1.running_var\", \"layer2.2.bn1.num_batches_tracked\", \"layer2.2.conv2.weight\", \"layer2.2.bn2.weight\", \"layer2.2.bn2.bias\", \"layer2.2.bn2.running_mean\", \"layer2.2.bn2.running_var\", \"layer2.2.bn2.num_batches_tracked\", \"layer3.2.conv1.weight\", \"layer3.2.bn1.weight\", \"layer3.2.bn1.bias\", \"layer3.2.bn1.running_mean\", \"layer3.2.bn1.running_var\", \"layer3.2.bn1.num_batches_tracked\", \"layer3.2.conv2.weight\", \"layer3.2.bn2.weight\", \"layer3.2.bn2.bias\", \"layer3.2.bn2.running_mean\", \"layer3.2.bn2.running_var\", \"layer3.2.bn2.num_batches_tracked\". \n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([16, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 3, 7, 7]).\n\tsize mismatch for bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer1.0.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer1.0.bn2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.bn2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.bn2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.0.bn2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.conv1.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer1.1.bn1.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.bn1.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.bn1.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.bn1.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.conv2.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for layer1.1.bn2.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.bn2.bias: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.bn2.running_mean: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer1.1.bn2.running_var: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for layer2.0.conv1.weight: copying a param with shape torch.Size([32, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for layer2.0.bn1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.bn1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.bn1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.bn1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer2.0.bn2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.bn2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.bn2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.bn2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.0.weight: copying a param with shape torch.Size([32, 16, 1, 1]) from checkpoint, the shape in current model is torch.Size([128, 64, 1, 1]).\n\tsize mismatch for layer2.0.downsample.1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.0.downsample.1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.conv1.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer2.1.bn1.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.bn1.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.bn1.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.bn1.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.conv2.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for layer2.1.bn2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.bn2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.bn2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer2.1.bn2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for layer3.0.conv1.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 128, 3, 3]).\n\tsize mismatch for layer3.0.bn1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.bn1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.bn1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.bn1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer3.0.bn2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.bn2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.bn2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.bn2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.0.weight: copying a param with shape torch.Size([64, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([256, 128, 1, 1]).\n\tsize mismatch for layer3.0.downsample.1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.0.downsample.1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.conv1.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer3.1.bn1.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.bn1.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.bn1.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.bn1.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.conv2.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([256, 256, 3, 3]).\n\tsize mismatch for layer3.1.bn2.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.bn2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.bn2.running_mean: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for layer3.1.bn2.running_var: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([10, 64]) from checkpoint, the shape in current model is torch.Size([10, 512])."
     ]
    }
   ],
   "source": [
    "# CIFAR10 parameters\n",
    "if dataset_name == \"CIFAR10\":\n",
    "    dataset = datasets.Cifar(batch_size=batch_size, data_location=path.join(data_root, \"CIFAR10\"), download=False, shuffle=False, version=\"cifar10\")\n",
    "    model = models.Resnet(version=\"resnet18\", output_logits=use_logits, num_classes=10, params_loc=path.join(data_root, \"models/CIFAR10_resnet20.pth\"))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST parameters\n",
    "if dataset_name == \"MNIST\":\n",
    "    dataset = datasets.MNIST(batch_size=batch_size, data_location=path.join(data_root, \"MNIST\"), download=False, shuffle=False)\n",
    "    model = models.MNISTCNN(params_loc=path.join(data_root, \"models/MNIST_cnn.pth\"), output_logits=use_logits)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"normalize\": normalize_attrs,\n",
    "    \"aggregation_fn\": aggregation_fn\n",
    "\n",
    "}\n",
    "\n",
    "attribution_methods = {\n",
    "    \"GuidedGradCAM\": methods.GuidedGradCAM(model, model.get_last_conv_layer(), **kwargs),\n",
    "    \"Gradient\": methods.Gradient(model, **kwargs),\n",
    "    \"InputXGradient\": methods.InputXGradient(model, **kwargs),\n",
    "    \"IntegratedGradients\": methods.IntegratedGradients(model, **kwargs),\n",
    "    \"GuidedBackprop\": methods.GuidedBackprop(model, **kwargs),\n",
    "    \"Deconvolution\": methods.Deconvolution(model, **kwargs),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Batch 2\n",
      "Batch 3\n",
      "Batch 4\n",
      "Batch 5\n",
      "Batch 6\n",
      "Batch 7\n",
      "Batch 8\n",
      "Batch 9\n",
      "Batch 10\n",
      "Batch 11\n",
      "Batch 12\n",
      "Batch 13\n",
      "Batch 14\n",
      "Batch 15\n",
      "Batch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arne/Documents/PhD/projects/benchmark/venv/lib/python3.6/site-packages/captum/attr/_utils/gradient.py:33: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  \"required_grads has been set automatically.\" % index\n",
      "/home/arne/Documents/PhD/projects/benchmark/venv/lib/python3.6/site-packages/captum/attr/_core/guided_backprop_deconvnet.py:56: UserWarning: Setting backward hooks on ReLU activations.The hooks will be removed after the attribution is finished\n",
      "  \"Setting backward hooks on ReLU activations.\"\n"
     ]
    }
   ],
   "source": [
    "result = infidelity(data=itertools.islice(dataset.get_dataloader(train=False), n_batches), model=model,\n",
    "                    methods=attribution_methods, n_perturbations=n_perturbations, pixel_level=normalize_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame.from_dict(\n",
    "    {m_name: pd.Series(data=result[m_name]) for m_name in attribution_methods}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "GuidedGradCAM          164.828598\nGradient               169.017807\nInputXGradient         166.917404\nIntegratedGradients    170.609848\nGuidedBackprop         164.475479\nDeconvolution          182.588013\ndtype: float32"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}